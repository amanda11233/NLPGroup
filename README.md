# Analysis on attention of pretrained models (T5, BART)

#### Group Members:
- Amanda Raj Shrestha (st122245)
- Omer Farooq Bhatti
- Harold Popluhar

### Referenced Papers
What Does BERT Look At? An Analysis of BERT's Attention: 
https://arxiv.org/abs/1906.04341

Do Transformer Attention Heads Provide Transparency in Abstractive Summarization: 
https://arxiv.org/abs/1907.00570

BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension] 
https://arxiv.org/abs/1910.13461

BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: 
https://arxiv.org/abs/1810.04805

Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer: 
https://arxiv.org/abs/1910.10683

 

### Libraries Used

Hugging face: 
https://huggingface.co

Ecco: 
https://github.com/jalammar/ecco

BERTviz: 
https://github.com/jessevig/bertviz.

##### Code Reference
clarkkev/attention-analysis: 
https://github.com/clarkkev/attention-analysis

